\documentclass[french,12pt,a4paper]{article}
\usepackage{fontspec}
\usepackage[french]{babel}
\usepackage{microtype}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{tikz}

\usepackage[colorlinks]{hyperref}
\usepackage{acronym}
\usepackage{listings}
\setmainfont[Ligatures=TeX,Scale=1.1]{EB Garamond}
\setmonofont[Scale=1.0]{Fira Code}
\renewcommand{\footnotesize}{\fontsize{7pt}{9pt}\selectfont}

\let\oldFootnote\footnote
\newcommand\nextToken\relax

\renewcommand\footnote[1]{%
	\oldFootnote{#1}\futurelet\nextToken\isFootnote}

\newcommand\isFootnote{%
	\ifx\footnote\nextToken\textsuperscript{,}\fi}

\lstset{basicstyle=\fontsize{10pt}{11pt}\selectfont\ttfamily,breaklines=true}
\acrodef{DSL}{Domain Specific Language}
\acrodef{EDSL}{Embedded Domain Specific Language}
\title{Audio et i-score}
\author{Jean-Michaël Celerier}

\begin{document}

\maketitle

\section{Introduction}
Ce document présente différentes possibilités 
pour l'écriture et l'utilisation de son dans i-score, 
et les environnements qui peuvent lui être adjoints.

\subsection{Problématique}
Ce document pose plusieurs questions, et pour chacunes essaye d'apporter les différentes réponses possibles.
\begin{itemize}
    \item Quel doit être l'agencement des responsabilités entre i-score et les outils annexes pour avoir des possibilités d'écriture maximale.
    Cette question de partage de responsabilités se pose aussi au niveau des utilisateurs de ces outils : qui utilise i-score ? Qui utilise wWise ? Qui utilise Unity ? 
    \item Quelles sont les contraintes techniques qui peuvent s'appliquer lors de l'interopérabilité avec différents environnements, et comment les résoudre. 
    Par exemple, comment faire en sorte que l'environnement fonctionne sur mobile.
    \item Comment doit fonctionner la gestion des médias dans un projet complet.
    \item Comment doit fonctionner la répartition dans un cas ou l'on désire produire une \oe uvre distribuée; cela pose la question de la séparation du moteur d'édition et d'exécution dans le cas du son, ainsi que de leur communication.
    \item Quelles sont les problématiques d'écriture qui se pose lorsque l'on désire écrire des scènes sonores ? (Mute de certaines parties ? Collaboration à l'écriture ?)
    \item Un modèle de calcul a-t-il sa place dans l'environnement, et si oui, qui doit le fournir, et où ces calculs sont-ils écrits ?
\end{itemize}

% - Séparation écriture / exécution
% - facilité de l'écriture : côté GAF ? côté BY ? côté artistes ?
% - modèle de calcul ? veut-on écrire une cumulation d'effets dans i-score  ou bien avoir tous les calculs faits à l'extérieur ?

\section{Problématiques techniques}
\subsection{Fonctionnement sur mobile / plate-formes embarquées}
Il n'y a généralement pas ou peu d'IPC sur ces plate-formes : tout doit être contenu dans une seule application.
Exception : les applications audio sur iOS, depuis iOS 7\footnote{\url{https://developer.apple.com/library/ios/documentation/iPhone/Conceptual/iPhoneOSProgrammingGuide/Inter-AppCommunication/Inter-AppCommunication.html}}\footnote{\url{http://www.musicradar.com/tuition/tech/audiobus-vs-inter-app-audio-which-is-best-620144}}.
La méthode standard est de communiquer via internet.

Plusieurs approches : Audiobus, Ableton Link.

Pour Android il n'y a pas encore de standard pour le faire\footnote{\url{http://developer.samsung.com/galaxy\#professional-audio}}\footnote{\url{http://superpowered.com/}}\footnote{\url{http://discchord.com/blog/2013/9/4/patchfield-audio-architecture-for-android.html}}.

\section{Types de moteurs audio}
Il existe de nombreuses possibilités pour écrire une application 
produisant du son. 
Les outils que l'on considère peuvent se répartir dans différentes
catégories (et faire partie de plus d'une catégorie à la fois).
\begin{itemize}
    \item Application graphique (Max/MSP, Ableton Live)
    \item Bibliothèque, framework, API (PortAudio, Jamoma)
    \item Middleware (wWise, Unity)
    \item \ac{DSL} (FaUST, Max/MSP)
    \item Application de contrôle et de routage (Audiobus, JACK)
    \item Plug-in pour une autre application (ils sont nombreux)
\end{itemize}

Les interactions peuvent être nombreuses entre ces formats :
Par exemple, un middleware peut fournir une API pour intégrer des plug-ins.
Un de ces plug-ins peut être écrit dans un DSL qui est compilé en C.

Les API vont être de plusieurs types : 
\begin{itemize}
\item API d'intégration : pour rajouter des outils audio à un système complet (ex. : API VST).
\item API de création : pour créer ses propres outils audio (ex. : STK).
\end{itemize}

Un autre axe est la main que prend l'API sur la gestion du son : 
\begin{itemize}
\item API maîtresse : le programmeur n'a pas le contrôle du flot audio. Ex. : OpenAL, Unity.
\item API esclave : le programmeur contrôle le flot audio (il n'est pas géré par l'API). Ex. : PortAudio, API bas-niveau.
\end{itemize}

Certaines API peuvent fonctionner des deux manières.

On trouve de plus plusieurs modes d'exécution (qui là encore peuvent fonctionner de concert) : 
\begin{itemize}    
    \item Déclaratif : on décrit à l'avance les traitements qui vont avoir lieu dans un langage propre à l'API (souvent un \ac{EDSL}).
    \item Graphe audio : cas déclaratif le plus courant.
    On décrit la manière dont des blocs audio qui appliquent un traitement sont reliés entre eux. 
    \item Push : très utilisé dans les moteurs de jeu; l'utilisateur doit régulièrement appeler une fonction, 
    généralement dans la boucle \lstinline{update()}, qui va mettre à jour le son.
    \item Pull : l'API appelle régulièrement une fonction fournie par l'utilisateur qui remplit un buffer audio.
C'est l'approche la plus courante pour du code bas-niveau ou devant être performant car elle correspond 
au fonctionnement des drivers audio.
\end{itemize}


Autre possibilité intéressante issue de Faust : utiliser LLVM pour recompiler et optimiser
à la volée du patch ?
Sans cela, il est aussi possible d'envisager d'utiliser libpd.
\section{Modèle de données}
Protocole de communication pour informer des données audio ? 
Si par exemple on veut avoir des pistes en sortie d'i-score qui rentrent 
dans wWise ou autre, comment fait-on ? 
Tout mixer sur le nombre de canaux offerts ? Ou faire le mixage avec JACK ?

\section{Briques en présence}
Les outils dont on dispose sont : 
\begin{itemize}
    \item Des moteurs de scénarisation. Un moteur de scénarisation permet de décrire une évolution du temps, 
    en prenant en compte des évènements extérieurs. 
    i-score et wWise en sont.
    \item Des sources de données écrites (scènes) spatiales. 
    Ce sont des descriptions et agencements d'objets en deux ou trois dimensions qui peuvent évoluer. 
    Dans le cadre des moteurs de jeux vidéos, on parle souvent de World Builders.
    On peut aussi mettre dans ce cas les automations à une ou plusieurs dimensions (par exemple, splines d'animations).
    \item Des moteurs de son, tels que décrits précédemment.
    \item Des moteurs de physique : prennent à un instant t une scène spatiale et la transforment en une autre scène après application des lois physiques en vigueur (gravité, etc.).
    \item Des sources de données spatiales : GPS, etc.
\end{itemize}

\section{Possibilités d'implémentation}
Questions : 
\begin{itemize}
\item qui gère la sortie son ?
\item qui gère la spatialisation en sortie (sur les hauts-parleurs)
\item qui gère la spatialisation en entrée (des objets)
\item qui applique des effets
\item qui contrôle l'écoulement du temps scénaristique
\item qui fait office de source sonore
\item qui communique avec qui
\end{itemize}

En vrac : 
\begin{itemize}
\item SuperCollider comme moteur audio ?
\item Tout dans i-score ? 
\item libaudiostream et la place de FaUST ?
\item libpd?
\item Grapholine ?
\item Problème du contrôle si deux moteurs.
\end{itemize}

Une des questionq principale est : dans quel langage écrit-on le comportement 
sonore des objets interactifs ?
\subsection{Séquenceur intégré à i-score}
i-scor epourrait devenir un séquenceur audio complet.
Question principale : gestion de l'horloge.
L'ébauche d'implémentation réalisée dans le plugin : \lstinline|iscore-addon-audio|
correspond aux étapes suivantes : 
\begin{itemize}
\item Un processus s'enregistre auprès d'un mixeur global.
\item Quand le processus est démarré, le mixeur appelle \lstinline||pull()| dessus à son tick d'horloge.
\item Problématique : si le processus est déclenché via un évènement interactif, il est nécessaire d'avoir un méchanisme de synchronisation (par exemple en commençant à appeler pull dès que l'on est dans la zone interactive, ou bien en utilisant des mutex / compteurs atomiques, ou bien en rajoutant de la latence, ou bie en mangeant le début.)
\item Problématique : bufferisation si effet sonore met du temps à s'appliquer ou à démarrer.
\end{itemize}

Un exemple de processus a été réalisé par le biais de FaUST, et un autre permet de lire des fichiers WAV.
À chaque tick d'horloge, on va récupérer la sortie d'un plug-in écrit en FaUST.
On offre une API pour écrire nos propres processus audio; cette API pourrait offrir des spécialisations pour les plug-ins de type VST par exemple.

Pour cette approche, il est nécessaire de rajouter dans i-score : 
\begin{itemize}
    \item Une gestion plus poussées des médias, et une notion de projet plus forte.
    \item Une gestion des canaux, et des pistes sons.
    On pourrait imaginer avoir un process Piste audio, qui est une succession de contraintes, et sur lequel s'applique une chaîne d'effet, comme dans un séquenceur classique.
    Réfléchir à la notion de groupe : un processus audio auquel des effets sont appliqués les applique certainement à ses enfants.
    \item L'application des automations et des mappings sur les paramètres de ces effets et pistes audio.
\end{itemize}

\subsection{Utilisation d'un séquenceur sous forme de bibliothèque}
On pense ici à une bibliothèque dy type libaudiostream.
En revanche, pour l'instant cette bibliothèque n'offre pas de possibilités 
d'écriture à la volée, comme le font la majorité des séquenceurs.
C'est du au fait que l'on décrive un graphe, qui est compilé puis exécuté.

Ce graphe peut contenir une part d'interactivité, mais 
qui est moins puissante que celle d'i-score.

En revanche il permet de placer dans le temps des effets FaUST ainsi 
que de l'interactivité dans une certaine mesure (notion de data symbolique); et surtout de réutiliser à
la volée une entrée temps-réel.
Il pourrait donc être intéressant d'avoir une approche basée sur cette bibliothèque,
peut-être en réutilisant certains éléments du formalisme graphique d'i-score 
mais placés dans un processus répondant aux spécificités de la libaudiostream.

On peut aussi imaginer s'inspirer des briques de la libaudiostream qui ne sont pas présentes 
dans i-score, comme la gestion du multi-canal, que l'on pourrait réimplémenter en prenant en compte le graphe d'objets de i-score.

\subsection{Utilisation d'un séquenceur externe}
Ici, il sera nécessaire de trouver un séquenceur possédant un namespace par OSC, 
pour le contrôler depuis i-score.

Les avantages sont qu'il y a moins de code à produire s'il existe déjà un 
tel séquenceur.
Notamment, les environnements SuperCollider ou Reaper pourraient être utilisés
à cet effet. Dans les deux cas il sera nécessaire d'offrir un namespace 
OSC correspondant à leurs API respectives
\footnote{API Reaper : \url{http://www.reaper.fm/sdk/reascript/reascripthelp.html}}\footnote{Séquençage dans SuperCollider :~\\ \url{http://doc.sccode.org/Tutorials/Getting-Started/15-Sequencing-with-Routines-and-Tasks.html}}.

En revanche, les problèmes posés sont ceux de la latence du réseau, 
ainsi qu'une complexité accrue d'écriture pour le compositeur, 
qui doit alors réaliser un travail d'écriture dans un environnement ou il n'a pas 
accès directement aux paramètres du son.
Par exemple, il sera dur d'implémenter efficacement le visionnage d'une waveform.

Un autre example pourrait être avec Unity en tant que moteur de son.
Unity est un environnement orienté objet plutôt que orienté pistes.
Une implémentation possible pourrait être d'avoir une 
piste par objet.
Unity est capable d'utiliser les informations de position des objets
pour réaliser une spatialisation du son par rolloff linéaire.
Il est possible d'écrire un plug-in pour unity qui implémente de 
meilleurs algorithmes de spatialisation\footnote{\url{http://docs.unity3d.com/Manual/AudioSpatializerSDK.html}}.

La latence mesurée est cependant assez élevée : 120 millisecondes. 
La mesure de la latence s'est faite de la manière suivante : 
\begin{itemize}
	\item Mise en route de l'enregistrement.
	\item Clic sur un objet qui joue un son percussif.
	\item Mesure du temps entre le clic et le début du son.
\end{itemize}

Pour comparaison, sur la même machine, un enregistrement en duplex complet sur un DAW professionnel (on lit un son, on l'enregistre, et on mesure l'écart au son original) produit une latence de 70 millisecondes, c'est à dire qu'on peut supposer 35 millisecondes de latence d'entrée et 35 millisecondes de latence de sortie inhérentes à la carte son.
La latence de sortie propre à Unity est donc d'environ 85 millisecondes.

Cette latence risque de varier fortement en fonction des périphériques. 

\subsection{Utilisation de i-score à deux échelles}
\begin{figure}
    \centering
    \scalebox{0.5}{\input{images/unity-1}}
    \caption{Illustration des communications possibles entre i-score et Unity:
        Une flèche à pointe noire illustre une communication directe. Une flèche à pointe blanche illustre une communication réseau. Les logiciels sont en rond, les objets Unity en carré, les scripts unity en losange.}
    \label{fig.com.iscore.unity}
\end{figure}

i-score est utilisé à haut niveau pour des grands scénarios et à bas niveau pour le contrôle des objets sonores.
Entre les deux, prennent place des communications avec les objets qui gèrent le son.

La question importante est celle du contrôle : si on a plusieurs objets à scénariser, comment 
ensuite pouvoir contrôler le démarrage de leur scénarisation depuis l'extérieur ? 

Cet extérieur peut être un autre gestionnaire de scénario (comme une instance de i-score).
Il est aussi possible de peut cacher l'API interne d'i-score derrière une API à utiliser dans Unity pour avoir un environnement entièrement intégré. 
Cela est nécessaire pour le portage vers certaines plate-formes. 
Ce cas est semblable à celui de l'objet 1 en fig.~\ref{fig.com.iscore.unity}, mais les flèches qui partent du i-score maître partent alors du player i-score.

Une autre possibilité est que i-score, ou bien un autre logiciel dédié audio (Grapholine ?), soit un plug-in qui puisse s'intégrer avec les API des différents logiciels audio comme wWise; c'est le cas de l'objet 2 en fig.~\ref{fig.com.iscore.unity}.
La question se pose, dans le cas de i-score, d'un choix d'outil audio permettant de s'intégrer à un séquenceur extérieur.

On pourrait supposer que dans ce cas, l'horloge d'exécution serait donnée par Unity. 
La durée d'un tick serait donc celle d'une frame graphique : généralement 16 millisecondes.

Enfin, l'implémentation actuelle est donnée dans l'objet 3 : il n'y a pas d'i-score "sous" Unity; le contrôle est fait par un i-score extérieur qui peut contrôler les propriétés de tout objet Unity. 

Un travail est aussi nécessaire sur la gestion automatique des ports, s'il y a plusieurs instances d'i-score qui communiquent directement par réseau, ou bien un travail d'identification est nécessaire si la communication est dirigée par l'horloge d'Unity.

\subsection{Intégration de i-score dans Unity}
Possibilités :
\begin{itemize}
\item avoir des éléments de la vue d'i-score intégrés aux objets unity
\item avoir des morceaux de vue Qt dans les panneaux unity, en faisant un rendu dans un FBO
\item échange de données à haut niveau : si par exemple on a des trajectoires dans unity, pouvoir aussi les visualiser dans i-score et réciproquement
\item rendu de parties de la scène unity dans fenêtre i-score (nécessite de voir comment on fait un rendu opengl dans QGraphicsScene) : cela peut ouvrir la porte à du rendu de parties de Unity dans i-score\footnote{\url{http://answers.unity3d.com/questions/871077/how-to-display-unity-scene-in-a-qt-window.html}}. Sinon il est possible de voir avec Qt3D.
\end{itemize}

De manière générale pour l'implémentation, si une brique audio est réalisée, la communication avec d'autres environnements doit être d'une importance extrême.

Pour ce faire, le principe est généralement d'écrire une fonction de la forme : 
\begin{lstlisting}
    void compute(float** in, float** out, int channel_size, int buffer_size);
\end{lstlisting}
Il y a généralement des paramètres à contrôler : chaque API (Audio Unit, VST\dots) a sa propre manière d'exprimer ces paramètres.

Si i-score lui même devient la brique audio, il pourrait être possible de : 
\begin{itemize}
    \item Utiliser i-score pour héberger d'autres plug-ins audio (par exemple dans les processus).
    \item Utiliser i-score en tant que plug-in dans d'autres applications (en l'intégrant dans la chaîne de traitements de Max, Unity, Pd, etc).
\end{itemize}

Cela implique une séparation dans le moteur d'exécution entre le back-end audio (ex. RtAudio, JACK, PortAudio), et l'exécution elle-même. 

Pb. de la bufferistaion : temps-réel, latence ?

Question de la license pour des extensions wWise et Unity ? 
Il existe des extensions sous licenses libres : 
\begin{itemize}
	\item LGPL : https://github.com/dblstallion/s3eWwise
	\item MIT : https://github.com/CaKlassen/gmwwise
\end{itemize}


\subsection{Export de scénario en tant que module}
Possibilité : compilation d'un scénario i-score via LLVM en 
un objet exécutable.

Cela aurait pour conséquence la possibilité d'exécuter de tels 
scénarios dans tout environnement ciblé par LLVM, comme par 
exemple une page web.

Cependant il est nécessaire dans ce cas de voir l'intégration 
avec Unity.

Il faut aussi gérer le case de la communication réseau qui 
est dans ce cas restreinte aux WebSockets (avec la nécessité de 
passer par l'API emscripten, et bientôt WebAssembly). 
De même pour l'audio et le midi : il faudra utiliser Web Audio API\footnote{\url{http://webaudio.github.io/web-audio-api/}}, et Web MIDI API\footnote{\url{https://webaudio.github.io/web-midi-api/}}.

Unity s'exporte aussi sur toutes les plate-formes.
Cependant sur certaines plate-formes les extensions natives ne sont 
pas supportées.

Une possibilité : compiler i-score en JavaScript via emscripten, 
et l'utiliser en tant que script Unity ? 

\section{Questions sémantiques}
Possibilté d'utilisation d'un outil comme OWL ?

\section{Applications possibles}
\begin{itemize}
\item Tableaux, installations, etc.
\item Morceaux de musiques interactifs que l'on peut distribuer par internet.
\item Jeux vidéos musicaux.
\item Aide au handicap.
\end{itemize}

\end{document}
